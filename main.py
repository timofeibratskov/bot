import openai
from pathlib import Path
import base64
import requests
from io import BytesIO
from aiogram import Bot, Dispatcher, executor, types
from aiogram.types import ContentType, File, Message,Voice,ReplyKeyboardMarkup,InlineKeyboardMarkup,InlineKeyboardButton
from config import OPENAI_TOKEN, BOT_TOKEN
from gtts import gTTS
from aiogram.dispatcher.filters import Command
from aiogram.dispatcher import FSMContext
from aiogram.dispatcher.filters.state import State, StatesGroup

# Initialize OpenAI client
client = openai.OpenAI(api_key=OPENAI_TOKEN)
bot = Bot(token=BOT_TOKEN)
dp = Dispatcher(bot)
api_key = OPENAI_TOKEN

kb = ReplyKeyboardMarkup(resize_keyboard=True)
kb.add('–¥–∞–π —Å–æ–≤–µ—Ç, —á—Ç–æ –±—ã —Ç—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª –ø–æ–µ—Å—Ç—å?')


def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

#start command
@dp.message_handler(commands=['start'])
async def send_welcome(message: types.Message):
    await message.reply(
        f"–ü—Ä–∏–≤–µ—Ç, {message.from_user.full_name}!\n–Ø –±–æ—Ç-–Ω—É—Ç—Ä–∏—Ü–∏–æ–ª–æ–≥ üçΩ \n –û—Ç–ø—Ä–∞–≤—å –º–Ω–µ —Ñ–æ—Ç–æ –µ–¥—ã –∏ —è –ø–æ–¥—Å—á–∏—Ç–∞—é –∫–∞–ª–æ—Ä–∏–∏ –∏–ª–∏ –∑–∞–¥–∞–π –º–Ω–µ –≤–æ–ø—Ä–æ—Å,\n –∫—Å—Ç–∞—Ç–∏,–µ—Å–ª–∏ –ª–µ–Ω—å –ø–∏—Å–∞—Ç—å –º–æ–∂–µ—à—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ) ",
        reply_markup=kb
    )
#handler for photo
@dp.message_handler(content_types=['photo', 'document'])
async def handle_photo_message(message: types.Message):
    await message.photo[-1].download('photo.jpg')
    image_path = 'photo.jpg'
    base64_image = encode_image(image_path)
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    payload = {
        "model": "gpt-4-turbo",
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "—Å–∫–æ–ª—å–∫–æ –∏—Ç–æ–≥–æ–≤–∞—è —Å—É–º–º–∞ –±–∂—É –∏ –∫–∞–ª–æ—Ä–∏–π –≤ –µ–¥–µ –Ω–∞ —Ñ–æ—Ç–æ?"
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
    }

    response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload)

    await message.reply(response.json()['choices'][0]['message']['content'])
#handler for answers
@dp.message_handler()
async def get_answer(message: types.Message):
    response = await process_question(message.text)
    await message.reply(response)
#handler for voice
@dp.message_handler(content_types=['voice'])
async def handle_voice(message: types.Message):
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥–æ–ª–æ—Å–æ–≤–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏
        file_id = message.voice.file_id
        file = await bot.get_file(file_id)
        file_path = file.file_path
        destination = f'/mnt/data/{file_id}.ogg'
        # –°–∫–∞—á–∏–≤–∞–µ–º –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        await bot.download_file(file_path,destination)

        audio_file = open(destination, "rb")
        translation = client.audio.translations.create(
            model="whisper-1",
            file=audio_file
        )
        text_from_audio = translation.text
        response_in_text = await process_question(text_from_audio)
        tts = gTTS(response_in_text,lang='ru')
        audio_f = '/mnt/data/temp_audioanswer.ogg'
        tts.save(audio_f)
        with open(audio_f,'rb') as f:
            await bot.send_voice(message.from_user.id,f)
    except Exception as e:
        await message.reply("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è.")
        print(e)




async def process_question(question):
    # Create an assistant
    assistant = client.beta.assistants.create(
        name="–Ω—É—Ç—Ä–∏—Ü–∏–æ–ª–æ–≥",
        instructions="–≤—ã –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –¥–æ–∫—Ç–æ—Ä-–Ω—É—Ç—Ä–∏—Ü–∏–æ–ª–æ–≥.–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å",
        tools=[{"type": "code_interpreter"}],
        model="gpt-4-1106-preview",
    )

    # Create a thread
    thread = client.beta.threads.create()

    # Create a user message in the thread
    message = client.beta.threads.messages.create(
        thread_id=thread.id,
        role="user",
        content=question,
    )

    # Run the thread and poll for completion
    run = client.beta.threads.runs.create_and_poll(
        thread_id=thread.id,
        assistant_id=assistant.id,
        instructions="–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –æ—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å –∫–æ—Ä–æ—Ç–∫–æ –∏ –Ω–µ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º!",
    )

    if run.status == "completed":
        # Get the list of messages in the thread
        messages = client.beta.threads.messages.list(thread_id=thread.id)

        # Extract and return the assistant's response
        response = ""
        for message in messages:
            if message.role == "assistant" and message.content[0].type == "text":
                response += message.content[0].text.value + " "

        # Delete the assistant
        client.beta.assistants.delete(assistant.id)

        return response.strip()

    return None

if __name__ == '__main__':
    executor.start_polling(dp, skip_updates=True)